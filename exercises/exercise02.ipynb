{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group \n",
        "- **ID**: <your group ID>\n",
        "\n",
        "- **Members**: \n",
        "    - <your name1>\n",
        "    - <your name2>\n",
        "    - <your name3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hand-in\n",
        "- Please hand in this notebook with your code implementation via Ilias \n",
        "- Please make sure that there is exactly **one** submission per group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task Description\n",
        "\n",
        "Write a custom fused ReLU class for 2D tensors, that implements **vector addition and ReLU**, with the **forward** pass. Then write a fused ReLU Triton kernel for 2D tensors that performs the **vector addition and ReLU** with the **forward** pass and computes its matching **backward** pass. Verify that both implementations produce the same outputs and gradients on random inputs and write a benchmark test with 'triton.testing' to showcase the efficiency of Triton in comparison to Torch. And you should look into **PyTorch's JIT** to make your pytorch implementation more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grading scheme\n",
        "Total: 5 points\n",
        "1. **Implementation of the forward pass for the fused ReLU with PyTorch** (0.5 points)\n",
        "2. **Implementation of the forward & backward pass for the fused ReLU with Triton** (4 points)\n",
        "3. **Verify outputs & benchmark test** (0.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7_u2od9bODIg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton.language as tl\n",
        "import triton\n",
        "import triton.testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Implementation of the forward pass for the fused ReLU with PyTorch (0.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Implementation of the forward & backward pass for the fused ReLU with Triton (4 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Verify outputs & benchmark test (0.5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "study",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
