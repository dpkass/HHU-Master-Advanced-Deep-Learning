{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea1998b",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854f7b7",
   "metadata": {},
   "source": [
    "## Group \n",
    "- **ID**: <your group ID>\n",
    "\n",
    "- **Members**: \n",
    "    - <your name1>\n",
    "    - <your name2>\n",
    "    - <your name3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f91a77",
   "metadata": {},
   "source": [
    "## Hand-in\n",
    "- Please hand in this notebook with your code implementation via Ilias \n",
    "- Please make sure that there is exactly **one** submission per group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9912ed",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise you will implement the `Speculative Decoding` algorithm for fast inference.\n",
    "The algorithm was proposed in the paper [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) as Algorithm 1. Please get familiar with the paper and the algorithm before starting the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cb37b",
   "metadata": {},
   "source": [
    "## Grading scheme\n",
    "Total: 5 points\n",
    "1. **Implementation of the algorithm** (4 points)\n",
    "2. **Integration of the algorithm into the inference pipeline** (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ea5031",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f34a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1fab8e",
   "metadata": {},
   "source": [
    "### 1. Implementation of the algorithm (4 points)\n",
    "- Implement the `Speculative Decoding` algorithm as described in the paper.\n",
    "- Utilize TopK sampling inside the algorithm to sample from the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_decoding_step(Mp, Mq, prefix_ids, gamma, tokenizer, device=\"cuda\", top_k=50):\n",
    "    ### Your code here ###\n",
    "    return\n",
    "    ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfcb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "draft_model_name =  \"gpt2\"\n",
    "target_model_name = \"gpt2-xl\" # if this model is to large, you can use a smaller one like \"gpt2-large\" or \"gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name)\n",
    "target_model = AutoModelForCausalLM.from_pretrained(target_model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "draft_model = draft_model.to(device).eval()\n",
    "target_model = target_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665efb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Draft model parameters: {sum(p.numel() for p in draft_model.parameters())}\")\n",
    "print(f\"Target model parameters: {sum(p.numel() for p in target_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f91bf",
   "metadata": {},
   "source": [
    "### 2. Integration of the algorithm into the inference pipeline (1 point)\n",
    "Build an inference pipeline that uses the `Speculative Decoding` algorithm. Come up with some prefix which you can feed into the model as original input and generate some text with at least 100 tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
