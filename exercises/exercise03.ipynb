{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea1998b",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854f7b7",
   "metadata": {},
   "source": [
    "## Group \n",
    "- **ID**: <your group ID>\n",
    "\n",
    "- **Members**: \n",
    "    - <your name1>\n",
    "    - <your name2>\n",
    "    - <your name3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f91a77",
   "metadata": {},
   "source": [
    "## Hand-in\n",
    "- Please hand in this notebook with your code implementation via Ilias \n",
    "- Please make sure that there is exactly **one** submission per group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9912ed",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise, you will implement Supervised Finetuning (SFT) for the pretrained GPT-2 model. You should use the `transformers` library to load the pretrained model and tokenizer. You will finetune the model on the `Alpaca` dataset, which is a collection of instruction-following examples. The dataset can be found [here](https://huggingface.co/datasets/tatsu-lab/alpaca).\n",
    "Your implementation should contain the four parts specified below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cb37b",
   "metadata": {},
   "source": [
    "## Grading scheme\n",
    "Total: 5 points\n",
    "1. **Preparing the Dataloader** (1 point)\n",
    "2. **Sensible Configurations** (1 point)\n",
    "3. **Training loop** (2 points)\n",
    "4. **Generation of Question Answer pairs** (1 point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
