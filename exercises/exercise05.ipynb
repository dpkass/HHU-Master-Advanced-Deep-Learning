{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5",
   "id": "988c1e9fd6b06e68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Group \n",
    "- **ID**: <your group ID>\n",
    "\n",
    "- **Members**: \n",
    "    - <your name1>\n",
    "    - <your name2>\n",
    "    - <your name3>"
   ],
   "id": "f7a2c424a78b6b86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hand-in\n",
    "- Please hand in this notebook with your code implementation via Ilias \n",
    "- Please make sure that there is exactly **one** submission per group"
   ],
   "id": "5a4bb7c3be87d53b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise, you will implement the DPO algorithm and integrate it into a finetuning pipeline. The algorithm is described in the paper [DPO: Direct Preference Optimization for Language Models](https://arxiv.org/abs/2305.18290).\n",
    "You will use DPO to finetune a pretrained language model which is already aligned to conversational tasks.\n",
    "\n",
    "You will use the dataset published by Anthropic. The dataset is a collection of human preference comparisons between two model outputs. The dataset is available at [Huggingface (Anthropic's dataset)](https://huggingface.co/datasets/Anthropic/hh-rlhf)."
   ],
   "id": "2717f586b63efa65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Grading scheme\n",
    "Total: 5 points\n",
    "1. **Preparing the Dataloader** (1 point)\n",
    "2. **Loss Function** (1.5 points)\n",
    "3. **Choose a suitable Model** (0.5 points)\n",
    "4. **Training loop** (1.5 points)\n",
    "5. **Generation of Question Answer pairs** (0.5 points)"
   ],
   "id": "ae8a15a437b53a9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. ## Preparing the Dataloader (1 point) <br>\n",
    "Get familiar with the dataset and prepare the dataloader."
   ],
   "id": "a24a376725d4b17d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aec4c235ed0cf836"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2. ## Loss Function (1.5 points) <br>\n",
    "Implement the DPO loss function."
   ],
   "id": "25d12bd395651993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be6da1ee59319b09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. ## Choose a suitable Model (0.5 points) <br>\n",
    "Choose a **suitable** model **for the DPO finetuning**. You can use any pretrained model from the Huggingface Hub. Please justify your choice in 1-2 sentences."
   ],
   "id": "eb9abb48c55c737d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_name = \"___YOUR_MODEL_NAME___\"  # Replace with your model name",
   "id": "b295c94f42edf39a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "4. ## Training loop (1.5 points) <br>\n",
    "Implement the training loop. Which uses your dataloader, the DPO loss function and the model you chose. Also use sensible hyperparameters."
   ],
   "id": "78bad32673c87033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d20ec65f88a3f693"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "5. ## Generation of Question Answer pairs (0.5 points) <br>\n",
    "Generate question-answer pairs using the finetuned model and compare them to the base model."
   ],
   "id": "1017dda00cdce950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6e3104a1b030fb0"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
