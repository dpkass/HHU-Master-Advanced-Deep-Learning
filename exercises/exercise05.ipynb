{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exercise 5",
   "id": "988c1e9fd6b06e68"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Group\n",
    "- **ID**: 5\n",
    "\n",
    "- **Members**:\n",
    "    - Hasan Algafri\n",
    "    - Emre Dursunluer\n",
    "    - Taha El Amine Kassabi"
   ],
   "id": "f7a2c424a78b6b86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hand-in\n",
    "- Please hand in this notebook with your code implementation via Ilias \n",
    "- Please make sure that there is exactly **one** submission per group"
   ],
   "id": "5a4bb7c3be87d53b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task Description\n",
    "\n",
    "In this exercise, you will implement the DPO algorithm and integrate it into a finetuning pipeline. The algorithm is described in the paper [DPO: Direct Preference Optimization for Language Models](https://arxiv.org/abs/2305.18290).\n",
    "You will use DPO to finetune a pretrained language model which is already aligned to conversational tasks.\n",
    "\n",
    "You will use the dataset published by Anthropic. The dataset is a collection of human preference comparisons between two model outputs. The dataset is available at [Huggingface (Anthropic's dataset)](https://huggingface.co/datasets/Anthropic/hh-rlhf)."
   ],
   "id": "2717f586b63efa65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Grading scheme\n",
    "Total: 5 points\n",
    "1. **Preparing the Dataloader** (1 point)\n",
    "2. **Loss Function** (1.5 points)\n",
    "3. **Choose a suitable Model** (0.5 points)\n",
    "4. **Training loop** (1.5 points)\n",
    "5. **Generation of Question Answer pairs** (0.5 points)"
   ],
   "id": "ae8a15a437b53a9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.546276Z",
     "start_time": "2025-06-01T22:10:23.485826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm.auto import tqdm, trange"
   ],
   "id": "85ba2cff2463f85d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Preparing the Dataloader (1 point) <br>\n",
    "Get familiar with the dataset and prepare the dataloader."
   ],
   "id": "6798d9b2c09c3389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.641343Z",
     "start_time": "2025-06-01T22:10:28.636959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnthropicDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].values()"
   ],
   "id": "82a2daf0f82efc47",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.652682Z",
     "start_time": "2025-06-01T22:10:28.650020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_collate_fn(batch, tokenizer):\n",
    "    N = len(batch)\n",
    "    chosen, rejected = zip(*batch)\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        chosen + rejected,\n",
    "        padding='longest',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'chosen': {\n",
    "            'input_ids': encodings['input_ids'][:N],\n",
    "            'attention_mask': encodings['attention_mask'][:N],\n",
    "        },\n",
    "        'rejected': {\n",
    "            'input_ids': encodings['input_ids'][N:],\n",
    "            'attention_mask': encodings['attention_mask'][N:],\n",
    "        }\n",
    "    }"
   ],
   "id": "db05772d10c71c5b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.668799Z",
     "start_time": "2025-06-01T22:10:28.665498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(collate_fn, batch_size=8, num_workers=0):\n",
    "    ds = load_dataset('Anthropic/hh-rlhf')\n",
    "    train_ds, val_ds = AnthropicDataset(ds['train']), AnthropicDataset(ds['test'])\n",
    "    return (DataLoader(train_ds, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers, shuffle=True),\n",
    "            DataLoader(val_ds, collate_fn=collate_fn, num_workers=num_workers))"
   ],
   "id": "e43d28243d54d9df",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Loss Function (1.5 points) <br>\n",
    "Implement the DPO loss function."
   ],
   "id": "c9b6272e908314c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.687231Z",
     "start_time": "2025-06-01T22:10:28.684186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DPOLoss(nn.Module):\n",
    "    def __init__(self, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, input_ids_w, input_ids_l, logits_theta_w, logits_theta_l, logits_ref_w, logits_ref_l):\n",
    "        logp_theta_w = self.to_seq_logp(logits_theta_w, input_ids_w)\n",
    "        logp_theta_l = self.to_seq_logp(logits_theta_l, input_ids_l)\n",
    "        logp_ref_w = self.to_seq_logp(logits_ref_w, input_ids_w)\n",
    "        logp_ref_l = self.to_seq_logp(logits_ref_l, input_ids_l)\n",
    "\n",
    "        score_w = self.beta * (logp_theta_w - logp_ref_w)\n",
    "        score_l = self.beta * (logp_theta_l - logp_ref_l)\n",
    "\n",
    "        return -F.logsigmoid(score_w - score_l).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def to_seq_logp(logits, input_ids):\n",
    "        return F.log_softmax(logits, dim=-1).gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1).sum(dim=-1)"
   ],
   "id": "5d3086ae47c9f828",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 3: Choose a suitable Model (0.5 points) <br>\n",
    "Choose a **suitable** model **for the DPO finetuning**. You can use any pretrained model from the Huggingface Hub. Please justify your choice in 1-2 sentences."
   ],
   "id": "c0ebb73d9c4d3913"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.704479Z",
     "start_time": "2025-06-01T22:10:28.700402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Configuration:\n",
    "    def __init__(self):\n",
    "        self.num_epochs = 3\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "\n",
    "        self.model_name = 'gpt2-medium'\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model_theta = self.get_model()\n",
    "        self.model_ref = self.get_model()\n",
    "        for p in self.model_ref.parameters(): p.requires_grad = False\n",
    "\n",
    "        lr = 1e-6\n",
    "        self.optimizer = optim.AdamW(self.model_theta.parameters(), lr=lr)\n",
    "\n",
    "        self.criterion = DPOLoss(beta=1.0)\n",
    "\n",
    "        collate_fn = partial(custom_collate_fn, tokenizer=self.tokenizer)\n",
    "\n",
    "        num_workers = os.cpu_count() if self.device.type == 'cuda' else 0\n",
    "        batch_size = 8\n",
    "        self.train_dl, self.val_dl = load_data(collate_fn, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        self.model_path = '../models/ex05_best_model_min_loss.pt'\n",
    "\n",
    "    def get_model(self):\n",
    "        model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
    "        model.config.use_cache = True\n",
    "        return model"
   ],
   "id": "d4ab98974347988a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 4: Training loop (1.5 points) <br>\n",
    "Implement the training loop. Which uses your dataloader, the DPO loss function and the model you chose. Also use sensible hyperparameters."
   ],
   "id": "4e6e0da3441a8d32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:28.720718Z",
     "start_time": "2025-06-01T22:10:28.714187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model_theta, model_ref, train_dl, optimizer, criterion, device):\n",
    "    model_theta.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_dl):\n",
    "        chosen, rejected = batch['chosen'], batch['rejected']\n",
    "        w_input_ids = chosen['input_ids'].to(device)\n",
    "        w_attention_mask = chosen['attention_mask'].to(device)\n",
    "        l_input_ids = rejected['input_ids'].to(device)\n",
    "        l_attention_mask = rejected['attention_mask'].to(device)\n",
    "\n",
    "        logits_theta_w = model_theta(w_input_ids, attention_mask=w_attention_mask).logits\n",
    "        logits_theta_l = model_theta(l_input_ids, attention_mask=l_attention_mask).logits\n",
    "        with torch.inference_mode():\n",
    "            logits_ref_w = model_ref(w_input_ids, attention_mask=w_attention_mask).logits\n",
    "            logits_ref_l = model_ref(l_input_ids, attention_mask=l_attention_mask).logits\n",
    "\n",
    "        loss = criterion(logits_theta_w, logits_theta_l, logits_ref_w, logits_ref_l)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / len(train_dl)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def validate(model_theta, model_ref, val_dl, criterion, device):\n",
    "    model_theta.eval()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(val_dl):\n",
    "        chosen, rejected = batch['chosen'], batch['rejected']\n",
    "        chosen_input_ids = chosen['input_ids'].to(device)\n",
    "        chosen_attention_mask = chosen['attention_mask'].to(device)\n",
    "        rejected_input_ids = rejected['input_ids'].to(device)\n",
    "        rejected_attention_mask = rejected['attention_mask'].to(device)\n",
    "\n",
    "        logits_theta_chosen = model_theta(chosen_input_ids, attention_mask=chosen_attention_mask)\n",
    "        logits_ref_chosen = model_ref(chosen_input_ids, attention_mask=chosen_attention_mask)\n",
    "        logits_theta_rejected = model_theta(rejected_input_ids, attention_mask=rejected_attention_mask)\n",
    "        logits_ref_rejected = model_ref(rejected_input_ids, attention_mask=rejected_attention_mask)\n",
    "\n",
    "        loss = criterion(logits_theta_chosen, logits_ref_chosen, logits_theta_rejected, logits_ref_rejected)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(val_dl)\n",
    "\n",
    "\n",
    "def finetune(config):\n",
    "    dict_log = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    model_theta = config.model_theta.to(config.device)\n",
    "    model_ref = config.model_ref.to(config.device)\n",
    "\n",
    "    pbar = trange(config.num_epochs)\n",
    "    for epoch in pbar:\n",
    "        train_loss = train_one_epoch(model_theta, model_ref, config.train_dl, config.optimizer, config.criterion, config.device)\n",
    "        val_loss = validate(model_theta, model_ref, config.val_dl, config.criterion, config.device)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{config.num_epochs}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}')\n",
    "\n",
    "        dict_log['train_loss'].append(train_loss)\n",
    "        dict_log['val_loss'].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            os.makedirs(os.path.dirname(config.model_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_theta.state_dict(),\n",
    "                'optimizer_state_dict': config.optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "            }, config.model_path)"
   ],
   "id": "e3bfab0c5c41973c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:39.410670Z",
     "start_time": "2025-06-01T22:10:28.733239Z"
    }
   },
   "cell_type": "code",
   "source": "config = Configuration()",
   "id": "98c950b635d2097",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T22:10:51.211884Z",
     "start_time": "2025-06-01T22:10:39.425005Z"
    }
   },
   "cell_type": "code",
   "source": "finetune(config)",
   "id": "592e1c5315c26e60",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5409d439df74b429baf8c17fc982f4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/20100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c44aa86ced545668acc2f4357fd4c6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.71 GB, other allocations: 232.69 MB, max allowed: 18.13 GB). Tried to allocate 209.48 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mfinetune\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 59\u001B[39m, in \u001B[36mfinetune\u001B[39m\u001B[34m(config)\u001B[39m\n\u001B[32m     57\u001B[39m pbar = trange(config.num_epochs)\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     train_loss = \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_theta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_ref\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     60\u001B[39m     val_loss = validate(model_theta, model_ref, config.val_dl, config.criterion, config.device)\n\u001B[32m     62\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.num_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: Train Loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Val Loss = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mtrain_one_epoch\u001B[39m\u001B[34m(model_theta, model_ref, train_dl, optimizer, criterion, device)\u001B[39m\n\u001B[32m      8\u001B[39m l_input_ids = rejected[\u001B[33m'\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m'\u001B[39m].to(device)\n\u001B[32m      9\u001B[39m l_attention_mask = rejected[\u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m].to(device)\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m logits_theta_w = \u001B[43mmodel_theta\u001B[49m\u001B[43m(\u001B[49m\u001B[43mw_input_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mw_attention_mask\u001B[49m\u001B[43m)\u001B[49m.logits\n\u001B[32m     12\u001B[39m logits_theta_l = model_theta(l_input_ids, attention_mask=l_attention_mask).logits\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.inference_mode():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001B[39m, in \u001B[36mGPT2LMHeadModel.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[39m\n\u001B[32m   1054\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1055\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[32m   1056\u001B[39m \u001B[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001B[39;00m\n\u001B[32m   1057\u001B[39m \u001B[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001B[39;00m\n\u001B[32m   1058\u001B[39m \u001B[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001B[39;00m\n\u001B[32m   1059\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1060\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1062\u001B[39m transformer_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1063\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1064\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1068\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1069\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1070\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1071\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1072\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1073\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1074\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1075\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1076\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1077\u001B[39m hidden_states = transformer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1079\u001B[39m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001B[39m, in \u001B[36mGPT2Model.forward\u001B[39m\u001B[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    910\u001B[39m     outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    911\u001B[39m         block.\u001B[34m__call__\u001B[39m,\n\u001B[32m    912\u001B[39m         hidden_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    919\u001B[39m         output_attentions,\n\u001B[32m    920\u001B[39m     )\n\u001B[32m    921\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m922\u001B[39m     outputs = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    923\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    924\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    925\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    926\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    927\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    928\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    929\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    933\u001B[39m hidden_states = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    934\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:404\u001B[39m, in \u001B[36mGPT2Block.forward\u001B[39m\u001B[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001B[39m\n\u001B[32m    402\u001B[39m residual = hidden_states\n\u001B[32m    403\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.ln_1(hidden_states)\n\u001B[32m--> \u001B[39m\u001B[32m404\u001B[39m attn_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    405\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    406\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlayer_past\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    407\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    408\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    409\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    410\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    412\u001B[39m attn_output = attn_outputs[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# output_attn: a, present, (attentions)\u001B[39;00m\n\u001B[32m    413\u001B[39m outputs = attn_outputs[\u001B[32m1\u001B[39m:]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:335\u001B[39m, in \u001B[36mGPT2Attention.forward\u001B[39m\u001B[34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001B[39m\n\u001B[32m    331\u001B[39m     attn_output, attn_weights = \u001B[38;5;28mself\u001B[39m._upcast_and_reordered_attn(\n\u001B[32m    332\u001B[39m         query_states, key_states, value_states, attention_mask, head_mask\n\u001B[32m    333\u001B[39m     )\n\u001B[32m    334\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m335\u001B[39m     attn_output, attn_weights = \u001B[43mattention_interface\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    339\u001B[39m \u001B[43m        \u001B[49m\u001B[43mvalue_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    340\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    342\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn_dropout\u001B[49m\u001B[43m.\u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[32;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    343\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    347\u001B[39m attn_output = attn_output.reshape(*attn_output.shape[:-\u001B[32m2\u001B[39m], -\u001B[32m1\u001B[39m).contiguous()\n\u001B[32m    348\u001B[39m attn_output = \u001B[38;5;28mself\u001B[39m.c_proj(attn_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/homebrew/anaconda3/envs/advanced-deep-learning/lib/python3.11/site-packages/transformers/integrations/sdpa_attention.py:54\u001B[39m, in \u001B[36msdpa_attention_forward\u001B[39m\u001B[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.jit.is_tracing() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(is_causal, torch.Tensor):\n\u001B[32m     52\u001B[39m     is_causal = is_causal.item()\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m attn_output = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfunctional\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     63\u001B[39m attn_output = attn_output.transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m).contiguous()\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m attn_output, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: MPS backend out of memory (MPS allocated: 17.71 GB, other allocations: 232.69 MB, max allowed: 18.13 GB). Tried to allocate 209.48 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 5: Generation of Question Answer pairs (0.5 points) <br>\n",
    "Generate question-answer pairs using the finetuned model and compare them to the base model."
   ],
   "id": "e8963f10750edc9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def qa(config, max_batches=5):\n",
    "    for i, batch in enumerate(config.val_dl):\n",
    "        if i == max_batches: break\n",
    "\n",
    "        encoded = batch['input_ids'].to(config.device)\n",
    "        prompt = batch[\"prompts\"][0]\n",
    "        attention_mask = batch['attention_mask'].to(config.device)\n",
    "\n",
    "        gen_ids = config.model.generate(\n",
    "            encoded,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=config.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        out = config.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        assert out.startswith(prompt)\n",
    "\n",
    "        print(out)\n",
    "        print(\"-\" * 40)"
   ],
   "id": "aa45fac6aca62c1b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
